{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d253b80e-b860-437c-8a90-95e25819c01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import networkx as nx\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aba8e23-5338-4705-9943-91d8de835ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation parameters\n",
    "sigma = 1 #particle diameter\n",
    "r_cutoff = (2**-6)*sigma #interaction cutoff\n",
    "epsilon = 1e-23 #interaction strength\n",
    "v = 60 #self-propulsion\n",
    "lambda_tumble = 0.9 #tumble rate\n",
    "lambda_run = lambda_tumble \n",
    "num_steps = 100000 #number of steps in a simulations\n",
    "T = 1000 #final time\n",
    "num_particles = 10 #number of particles\n",
    "dt = 1e-5#T / num_steps #timestep\n",
    "bounds = int(num_particles/0.65) #bounds of the 1d box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0458e1-7231-48aa-857f-b17fa26d968a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulation functions\n",
    "\n",
    "def update_x(x, rho, v, dt, f, bounds):\n",
    "    \"\"\"\n",
    "    Update position based on velocity, run/tumble state, and force.\n",
    "    Applies periodic boundary conditions.\n",
    "    \"\"\"\n",
    "    x_new = x + rho * v * dt - f*dt\n",
    "    x_new = x_new % bounds  # Apply periodic boundary conditions\n",
    "    return x_new\n",
    "\n",
    "#removed the negative signs from force in above and below equations\n",
    "def calculate_force(epsilon, xi, xj, sigma, r_cutoff, bounds):\n",
    "    \"\"\"\n",
    "    Calculate force between two particles using the WCA potential.\n",
    "    \"\"\"\n",
    "    dx = xi - xj\n",
    "    dx = dx - np.round(dx / bounds) * bounds  # Apply periodic boundary conditions\n",
    "    r_ij = np.abs(dx)\n",
    "    r_min = 0.5*r_cutoff*2**3  #avoid division by zero\n",
    "\n",
    "    if r_ij < r_cutoff:\n",
    "        if r_ij <= r_min:\n",
    "            force = (24 * epsilon / r_min) * (2 * (sigma / r_min)**12 - (sigma / r_min)**6)\n",
    "            potential = (4 * epsilon ) * ( (sigma / r_min)**12 - (sigma / r_min)**6) + epsilon\n",
    "        else:\n",
    "            force = (24 * epsilon / r_ij) * (2 * (sigma / r_ij)**12 - (sigma / r_ij)**6)\n",
    "            potential = (4 * epsilon ) * ( (sigma / r_ij)**12 - (sigma / r_ij)**6) + epsilon\n",
    "    else:\n",
    "        force, potential = 0.0, 0.0\n",
    "\n",
    "    return force, potential\n",
    "\n",
    "def run_or_tumble(state, lambda_run, lambda_tumble, current_time, last_switch_time):\n",
    "    \"\"\"\n",
    "    Decide whether the particle continues its current state (run or tumble)\n",
    "    or switches based on the exponential distribution.\n",
    "    \"\"\"\n",
    "    if state == 1:  # Running\n",
    "        duration = np.random.exponential(1 / lambda_run)\n",
    "    else:  # Tumbling\n",
    "        duration = np.random.exponential(1 / lambda_tumble)\n",
    "\n",
    "    if current_time - last_switch_time >= duration:\n",
    "        state = -state  # Toggle state\n",
    "        last_switch_time = current_time  # Update the last switch time\n",
    "\n",
    "    return state, last_switch_time\n",
    "\n",
    "def enforce_no_exploding(x_new, positions, i, sigma, bounds):\n",
    "    \"\"\"\n",
    "    Enforce no unrealistic interactions between particles by checking the position.\n",
    "    \"\"\"\n",
    "    for k, xk in enumerate(positions):\n",
    "        if k != i:\n",
    "            dx = x_new - xk\n",
    "            dx = dx - np.round(dx / bounds) * bounds  # Apply periodic boundary conditions\n",
    "            r_ij = np.abs(dx)\n",
    "            packing_factor = 0.09\n",
    "            r_min = 0.5 * sigma * packing_factor #minimum distance to avoid divison by zero\n",
    "            if r_ij < r_min:\n",
    "                if dx > 0:\n",
    "                    x_new = xk + r_min\n",
    "                else:\n",
    "                    x_new = xk - r_min\n",
    "                x_new = x_new % bounds  # Ensure periodic boundary conditions\n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20743de3-7702-4eec-b8d2-ea1ff2a8b490",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulation set-up\n",
    "positions = np.zeros([num_steps, num_particles])\n",
    "positions[0, :] = positions[0, :] = np.linspace(0.5, bounds-0.5, num_particles) #np.random.uniform(0, bounds, num_particles)  # Initial positions\n",
    "force = np.zeros([num_steps, num_particles])\n",
    "potential = np.zeros([num_steps, num_particles])\n",
    "\n",
    "# Initialize run/tumble states and switch times for each particle\n",
    "states = np.ones(num_particles)  # Start all particles in \"run\" state\n",
    "last_switch_times = np.zeros(num_particles)  # Initialize last switch times\n",
    "\n",
    "# Initialize variables for run/tumble durations\n",
    "run_durations = [[] for _ in range(num_particles)]\n",
    "tumble_durations = [[] for _ in range(num_particles)]\n",
    "state_start_times = np.zeros(num_particles)  # Start time for the current state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7d6d28-3c5c-4e8a-bf5e-c4dc7a615ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main simulation loop with additional tracking\n",
    "for j in range(num_steps - 1):\n",
    "    current_time = j * dt  # Calculate current time\n",
    "    for i in range(num_particles):\n",
    "        # Update run/tumble state for the particle\n",
    "        prev_state = states[i]\n",
    "        states[i], last_switch_times[i] = run_or_tumble(\n",
    "            states[i], lambda_run, lambda_tumble, current_time, last_switch_times[i]\n",
    "        )\n",
    "        rho = states[i]\n",
    "\n",
    "        # Update run/tumble durations\n",
    "        if states[i] != prev_state:  # State change occurred\n",
    "            duration = current_time - state_start_times[i]\n",
    "            if prev_state == 1:  # Previous state was \"run\"\n",
    "                run_durations[i].append(duration)\n",
    "            else:  # Previous state was \"tumble\"\n",
    "                tumble_durations[i].append(duration)\n",
    "            state_start_times[i] = current_time  # Update start time for the new state\n",
    "\n",
    "        # Calculate forces between particles\n",
    "        force[j, i] = 0  # Reset force for particle i\n",
    "        potential[j, i] = 0\n",
    "\n",
    "        for k in range(num_particles):\n",
    "            if k != i:\n",
    "                f_wca, potential_wca = calculate_force(epsilon, positions[j, i], positions[j, k], sigma, r_cutoff, bounds)\n",
    "\n",
    "                force[j, i] += (f_wca*dt)\n",
    "                potential[j,i] += (potential_wca*dt)\n",
    "\n",
    "\n",
    "        # Update particle position\n",
    "        x_new = update_x(positions[j, i], rho, v, dt, force[j, i], bounds)\n",
    "\n",
    "        # Enforce no overlap condition\n",
    "        x_new = enforce_no_exploding(x_new, positions[j, :], i, sigma, bounds)\n",
    "\n",
    "        # Store the updated position\n",
    "        positions[j + 1, i] = x_new\n",
    "\n",
    "\n",
    "# Finalize results\n",
    "run_durations = [np.array(durations) for durations in run_durations]\n",
    "tumble_durations = [np.array(durations) for durations in tumble_durations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a595645f-f043-42e9-a118-696b5311bfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data for Animation\n",
    "animation_data = []\n",
    "\n",
    "for step in range(num_steps):\n",
    "    frame_data = pd.DataFrame({\n",
    "        'X': positions[step, :],\n",
    "        'Y': np.zeros(num_particles),  # If only 1D, we can place them along Y=0\n",
    "        'Particle': np.arange(num_particles),\n",
    "        'Size': [sigma] * num_particles,  # Scale size for visualization\n",
    "    })\n",
    "    frame_data['Time'] = step  # To enable smooth transition over time\n",
    "    animation_data.append(frame_data)\n",
    "\n",
    "df = pd.concat(animation_data).reset_index(drop=True)\n",
    "df[\"Particle\"] = df[\"Particle\"].astype(str)\n",
    "\n",
    "# Create Animation\n",
    "size_ref = sigma * 7  # Adjust size scaling for better visualization\n",
    "fig = px.scatter(\n",
    "    df.tail(1000), #remove .tail(..) if you want to start from t=0, BE WARNED that the animation is intensive so run for varying parameters carefully\n",
    "    x='X', y='Y', animation_frame='Time',\n",
    "    size='Size', size_max=size_ref,\n",
    "    title='Particle Simulation Animation',\n",
    "    range_x=[0, bounds], range_y=[-2, 2],  # Adjust Y-axis range for better 1D visualization\n",
    "    labels={'X': 'X Position', 'Y': 'Y Position'},\n",
    "    #color = 'Particle'\n",
    ")\n",
    "\n",
    "# Add boundary box for periodic conditions\n",
    "fig.add_shape(\n",
    "    type=\"rect\", x0=0, y0=-bounds/8, x1=bounds, y1=bounds/8,\n",
    "    line=dict(color=\"black\", width=2),\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    width=1000,\n",
    "    height=800,\n",
    "    yaxis_scaleanchor=\"x\",\n",
    "    title_text=\"1D Particle Motion with Periodic Boundaries\",\n",
    "    title_x=0.5,\n",
    ")\n",
    "\n",
    "# Show the animation\n",
    "#fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7e8c76-4a08-4ea5-8331-985554ff8d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Positions\n",
    "a,b = positions.shape\n",
    "list_positions = []\n",
    "\n",
    "for i in range(b):\n",
    "    # Create a DataFrame with X positions for each time step (i-th time step)\n",
    "    positionsDf = pd.DataFrame(data=positions[:, i], columns=['X'])\n",
    "\n",
    "    # Calculate the squared differences in the X direction\n",
    "    xDiff = positionsDf['X'] - positionsDf.at[0, 'X']\n",
    "    xDiffWrapped = bounds - np.abs(xDiff)  # Calculate the wrapped distance\n",
    "    positionsDf['xDiffSquared'] = np.minimum(xDiff**2, xDiffWrapped**2)  # Take the minimum of direct and wrapped squared distances\n",
    "    positionsDf['MSD'] = np.sqrt(positionsDf['xDiffSquared'])\n",
    "    positionsDf[\"dt\"] = dt\n",
    "    positionsDf[\"time\"] = positionsDf['dt'].cumsum()\n",
    "    positionsDf[\"Particle\"] = i\n",
    "    positionsDf[\"shift_1\"] = positionsDf[\"X\"].shift(1)\n",
    "    positionsDf.at[0,\"shift_1\"] = 0\n",
    "\n",
    "    # Append the result for the current time step\n",
    "    list_positions.append(positionsDf)\n",
    "distance = pd.concat(list_positions).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b21d2ed-f39b-42a4-aabc-565035e1d9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSD\n",
    "msd = distance.groupby(\"time\").mean()[\"MSD\"].reset_index()\n",
    "plt.loglog(msd['time'],msd['MSD'], linestyle='-', color='b')\n",
    "plt.title('Log-Log plot of the time vs average')\n",
    "plt.xlabel('Time (units)')\n",
    "plt.ylabel('Average (units)')\n",
    "plt.grid(True)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887e6047-488a-4b18-8498-78e3b499ac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the lists of durations to create a combined histogram\n",
    "all_run_durations = np.concatenate(run_durations)\n",
    "all_tumble_durations = np.concatenate(tumble_durations)\n",
    "\n",
    "# Calculate Kernel Density Estimate (KDE) for smoother line plots\n",
    "kde_run = gaussian_kde(all_run_durations)\n",
    "kde_tumble = gaussian_kde(all_tumble_durations)\n",
    "\n",
    "# Define a range for the x-axis based on data\n",
    "x_run = np.linspace(min(all_run_durations), max(all_run_durations), 1000)\n",
    "x_tumble = np.linspace(min(all_tumble_durations), max(all_tumble_durations), 1000)\n",
    "\n",
    "# Plot histograms with line plots\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Histogram and KDE for run durations\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(all_run_durations, bins=30, color='blue', alpha=0.7, label='Run Durations', density=True)\n",
    "plt.plot(x_run, kde_run(x_run), color='darkblue', lw=2)\n",
    "plt.xlabel('Duration')\n",
    "plt.ylabel('Density')\n",
    "plt.title('(a) Histogram and Line Plot of Run Durations')\n",
    "plt.legend()\n",
    "\n",
    "# Histogram and KDE for tumble durations\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(all_tumble_durations, bins=30, color='red', alpha=0.7, label='Tumble Durations', density=True)\n",
    "plt.plot(x_tumble, kde_tumble(x_tumble), color='darkred', lw=2)\n",
    "plt.xlabel('Duration')\n",
    "plt.ylabel('Density')\n",
    "plt.title('(b) Histogram and Line Plot of Tumble Durations')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddfe4ce-5a9c-4050-a002-ee0597b5bf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_list = []\n",
    "for i in range(num_particles):\n",
    "  slice_dataset = distance[distance[\"Particle\"] == i][[\"time\",\"Particle\",\"MSD\"]].copy()\n",
    "  slice_dataset[\"shift_1\"] = slice_dataset[\"MSD\"].shift(1)\n",
    "  slice_dataset[\"MSD_potential\"] = slice_dataset[\"MSD\"]*v\n",
    "  slice_dataset = slice_dataset.fillna(0)\n",
    "  slice_dataset[\"potential_delta\"] = np.abs(v*(slice_dataset[\"MSD\"] - slice_dataset[\"shift_1\"]))\n",
    "  slice_dataset = slice_dataset[[\"time\",\"Particle\",\"MSD_potential\",\"potential_delta\"]]\n",
    "  distance_list.append(slice_dataset)\n",
    "distance_potential_dataset = pd.concat(distance_list,axis=0).reset_index(drop=True)\n",
    "\n",
    "\n",
    "potential_list = []\n",
    "for i in range(num_particles):\n",
    "  potential_data = pd.DataFrame(data=potential[:,i], columns=[\"Energy\"]) #chenged potential to force\n",
    "  potential_data[\"Particle\"] = i\n",
    "  potential_data[\"time\"] = distance[\"time\"]\n",
    "  potential_data[\"shift_1\"] = potential_data[\"Energy\"].shift(1)\n",
    "  potential_data[\"wca_delta\"] = (np.abs(potential_data[\"Energy\"] - potential_data[\"shift_1\"]))*1e12\n",
    "  potential_data = potential_data[[\"time\",\"Particle\",\"Energy\",\"wca_delta\"]]\n",
    "  potential_data = potential_data.fillna(0)\n",
    "  potential_list.append(potential_data)\n",
    "\n",
    "\n",
    "potential_df = pd.concat(potential_list,axis=0).reset_index(drop=True)\n",
    "\n",
    "effective_potential_dataset = potential_df.merge(\n",
    "    distance_potential_dataset,\n",
    "    left_on = [\"time\",\"Particle\"],\n",
    "    right_on = [\"time\",\"Particle\"],\n",
    "    how = \"inner\"\n",
    ")\n",
    "\n",
    "effective_potential_dataset[\"wca_delta\"] = np.where((effective_potential_dataset[\"time\"] == \tdt) | (effective_potential_dataset[\"time\"] == 2*dt),0,effective_potential_dataset[\"wca_delta\"])\n",
    "effective_potential_dataset[\"potential_delta\"] = np.where((effective_potential_dataset[\"time\"] == \tdt) | (effective_potential_dataset[\"time\"] == 2*dt),0,effective_potential_dataset[\"potential_delta\"])\n",
    "effective_potential_dataset[\"effective_potential\"] = effective_potential_dataset[\"wca_delta\"] - effective_potential_dataset[\"potential_delta\"]\n",
    "effective_potential_dataset[\"shift_1\"] = effective_potential_dataset[\"effective_potential\"].shift(1)\n",
    "effective_potential_dataset[\"effective_delta\"] = effective_potential_dataset[\"effective_potential\"] - effective_potential_dataset[\"shift_1\"]\n",
    "effective_potential_dataset[\"avalanching\"] = np.where(effective_potential_dataset[\"effective_delta\"] < 0, 1, 0)\n",
    "effective_potential_dataset[\"avalanching\"] = np.where((effective_potential_dataset[\"time\"] == 2*dt) | (effective_potential_dataset[\"time\"] == 3*dt), 0, effective_potential_dataset[\"avalanching\"])\n",
    "effective_potential_dataset = effective_potential_dataset.fillna(0)\n",
    "\n",
    "\n",
    "avalanche_size_dataset = effective_potential_dataset.pivot(\n",
    "    index = \"time\",\n",
    "    columns = \"Particle\",\n",
    "    values = \"avalanching\"\n",
    ").reset_index().rename_axis(None, axis=1).set_index(\"time\")\n",
    "avalanche_size_dataset[\"avalanche_size\"] = avalanche_size_dataset.sum(axis=1)\n",
    "avalanche_size_dataset = avalanche_size_dataset.reset_index()\n",
    "avalanche_size_dataset[\"avalanche_size\"] = np.where(avalanche_size_dataset[\"time\"] == dt, 0, avalanche_size_dataset[\"avalanche_size\"])\n",
    "\n",
    "#Two point correlation\n",
    "\n",
    "# Filter and calculate time-averaged covariance\n",
    "df = avalanche_size_dataset[avalanche_size_dataset[\"avalanche_size\"] != 0].copy()\n",
    "df['S_t'] = df['avalanche_size']\n",
    "df['S_t+1'] = df['avalanche_size'].shift(-1)\n",
    "df = df.dropna()\n",
    "\n",
    "N = len(df)\n",
    "mean_S_t = df['S_t'].sum() / N\n",
    "mean_S_t_plus_1 = df['S_t+1'].sum() / N\n",
    "mean_S_t_S_t_plus_1 = (df['S_t'] * df['S_t+1']).sum() / N\n",
    "\n",
    "cov_S_t_S_t_plus_1 = mean_S_t_S_t_plus_1 - (mean_S_t * mean_S_t_plus_1)*2\n",
    "\n",
    "plot_effective_potential = effective_potential_dataset.groupby(\"time\").sum().reset_index().copy()\n",
    "plot_effective_potential[\"effective_delta\"] = np.where(plot_effective_potential[\"time\"] == dt,0,plot_effective_potential[\"effective_delta\"])\n",
    "plot_effective_potential[\"avalanching\"] = np.where(plot_effective_potential[\"time\"] == dt,0,plot_effective_potential[\"avalanching\"])\n",
    "plot_effective_potential = plot_effective_potential[[\"time\",\"effective_delta\",\"avalanching\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c5961b-91fc-44e2-a462-5fedf124286a",
   "metadata": {},
   "outputs": [],
   "source": [
    "avalanches = avalanche_size_dataset[[\"time\",\"avalanche_size\"]].copy()\n",
    "avalanches = avalanches.rename(columns={\"avalanche_size\":\"avalanche\"})\n",
    "avalanches[\"dt\"] = dt\n",
    "\n",
    "# Calculate the sum of dt for consecutive avalanche events\n",
    "avalanche_rows = avalanches[avalanches['avalanche'] == 1]\n",
    "avalanche_durations = []\n",
    "current_duration = 0\n",
    "\n",
    "for idx, row in avalanche_rows.iterrows():\n",
    "    # Add dt to the current duration if it's part of the avalanche\n",
    "    current_duration += row['dt']\n",
    "\n",
    "    # If the next row doesn't have avalanche = 1, save the current duration and reset\n",
    "    if idx + 1 not in avalanche_rows.index:\n",
    "        avalanche_durations.append(current_duration)\n",
    "        current_duration = 0  # Reset for next avalanche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497eb1dc-c253-4937-80a8-78b664494342",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Average Avalanche Size: {avalanche_size_dataset[\"avalanche_size\"].mean()}')\n",
    "print(f'Average Avalanche Duration: {np.mean(avalanche_durations)}')\n",
    "print(f'Two-Point Correlation: {cov_S_t_S_t_plus_1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c10bad4-f2ed-4781-827e-9a90e4e4be00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF plot (Probability Distribution Function) on a log-log scale\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot PDF on a log-log scale\n",
    "sns.histplot(avalanche_durations, bins=10, kde=False, stat=\"probability\", color='blue')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.title('Power-Law Distribution: Avalanche Durations (PDF)')\n",
    "plt.xlabel('Avalanche Duration (log scale)')\n",
    "plt.ylabel('Probability (log scale)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567b4292-a12d-4c70-b936-93e1033bb499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLot of Avalanche sizes the histogram\n",
    "plot_avalanche_sizes = avalanche_size_dataset.copy()\n",
    "plot_avalanche_sizes[\"avalanche_size\"] = np.where(plot_avalanche_sizes[\"time\"] < 0.1, 0,plot_avalanche_sizes[\"avalanche_size\"])\n",
    "plot_avalanche_sizes = plot_avalanche_sizes[[\"time\",\"avalanche_size\"]]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(30, 15))\n",
    "skip = 70\n",
    "subset_time = plot_avalanche_sizes[\"time\"][::skip]  # Select every 100th time value\n",
    "subset_avalanche_size = plot_avalanche_sizes[\"avalanche_size\"][::skip]  # Select every 100th avalanche size\n",
    "plt.plot(subset_time,subset_avalanche_size)#,linewidth=0.8)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Avlanche Size\")\n",
    "plt.title(f\"Sample Time Series Plot of Avalanche Sizes (System = {num_particles}) \")\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bbc7c6-e17e-4b91-b06e-2b1658d9f930",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot PDF on a log-log scale\n",
    "sns.histplot(avalanche_size_dataset[\"avalanche_size\"], bins=10, kde=False, stat=\"probability\", color='blue')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.title('Power-Law Distribution: Avalanche Size (PDF)')\n",
    "plt.xlabel('Avalanche Size (log scale)')\n",
    "plt.ylabel('Probability (log scale)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64185f40-616e-4f15-af5d-da312ca67e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "avalanche_sizes = avalanche_size_dataset[avalanche_size_dataset[\"avalanche_size\"] != 0].copy()\n",
    "\n",
    "# Define bins\n",
    "bins = np.logspace(np.log10(min(avalanche_sizes[\"avalanche_size\"])), np.log10(max(avalanche_sizes[\"avalanche_size\"])), num=6)\n",
    "\n",
    "# Compute histogram and normalize\n",
    "hist, bin_edges = np.histogram(avalanche_sizes[\"avalanche_size\"], bins=bins, density=True)\n",
    "\n",
    "# Calculate bin centers for plotting\n",
    "bin_centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])\n",
    "\n",
    "\n",
    "\n",
    "# Define a power-law function for fitting\n",
    "def power_law(x, tau, A):\n",
    "    return A * x**(-tau)\n",
    "\n",
    "# Fit the power-law region\n",
    "scaling_region = bin_centers < np.percentile(bin_centers, 95)  # Fit up to 90% of data\n",
    "\n",
    "# Estimate the characteristic avalanche size (s_c)\n",
    "s_c = bin_centers[np.argmax(hist * (bin_centers >= np.percentile(bin_centers, 90)))]\n",
    "print(f\"Characteristic avalanche size (s_c): {s_c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5b6de7-5e67-4d4f-81b8-26193336f016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store results\n",
    "cluster_results = []\n",
    "\n",
    "# Iterate over each time step\n",
    "for time, group in distance.groupby(\"time\"):\n",
    "    # Extract particle positions from the 'Position' column\n",
    "    positions = group[[\"X\"]].values  # Assuming 'Position' contains the position data\n",
    "    \n",
    "    # Compute pairwise distances between particles\n",
    "    distances = squareform(pdist(positions, metric=\"euclidean\"))\n",
    "    \n",
    "    # Create adjacency matrix (1 if distance <= r_cutoff, else 0)\n",
    "    adjacency_matrix = (distances <= r_cutoff+0.045).astype(int)\n",
    "    \n",
    "    # Use scipy's connected_components to find clusters\n",
    "    n_clusters, labels = connected_components(adjacency_matrix, directed=False)\n",
    "    \n",
    "    # Append results for all particles at this time step\n",
    "    cluster_results.extend(\n",
    "        {\"time\": time, \"Particle\": particle, \"Cluster\": cluster}\n",
    "        for particle, cluster in zip(group[\"Particle\"], labels)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b232ab67-1e65-49de-b026-b65079eee9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = pd.DataFrame(cluster_results)\n",
    "# Group by time and cluster to calculate cluster sizes\n",
    "cluster_sizes = cluster_df.groupby([\"time\", \"Cluster\"]).size().reset_index(name=\"ClusterSize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789177e4-ab30-469b-b7db-058a98ca2f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of Cluster Sizes\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot a histogram on a log-log scale\n",
    "sns.histplot(cluster_sizes[\"ClusterSize\"], bins=4, kde=False, color=\"blue\", edgecolor=\"black\")\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Cluster Size\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Cluster Size Distribution in a System of 21 Particles\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c5077a-1593-4f50-9bf9-5251cbabb275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique clusters per time step\n",
    "unique_clusters_per_time = cluster_df.groupby(\"time\")[\"Cluster\"].nunique().reset_index(name=\"UniqueClusters\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot a histogram of unique clusters\n",
    "sns.histplot(unique_clusters_per_time[\"UniqueClusters\"], bins=20, kde=False, color=\"blue\")\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Number of Unique Clusters\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Unique Clusters Across Time in a System of 21 Particles\")\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
